\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[usenames,dvipsnames]{color}
\usepackage{graphicx}
\usepackage[justification=centering,labelfont=bf]{caption}
\usepackage{listings}
\usepackage{minted}
\usepackage[hidelinks]{hyperref}
\begin{document}
\begin{titlepage}
\begin{center}
\textsc{\Large Parallelism}
\\
\texttt{1202}
\\[1.5cm]
\rule{\linewidth}{0.5mm}
\\[0.4cm]
{\huge
\bfseries
Lab 3: Divide and conquer parallelism with OpenMP - Sorting
\\[0.4cm]
}
\rule{\linewidth}{0.5mm}
\\[2.5cm]
\begin{minipage}{0.4\textwidth}
\begin{flushleft}
\large
Héctor Ramón Jiménez
\end{flushleft}
\end{minipage}
\begin{minipage}{0.4\textwidth}
\begin{flushright}
\large
Alvaro Espuña Buxó
\end{flushright}
\end{minipage}
\vfill
{\large
\today
}
\\
{\large
\texttt{Facultat d'Informàtica de Barcelona}
}
\end{center}
\end{titlepage}
\section{Analysis with Tareador}
\begin{enumerate}
\setcounter{enumi}{0}
\item
\textbf{Include the source of the sequential multisort.c code modified
  with the calls to the Tareador API and the task graph generated.}

As we can see in {\tt multisort-tareador.c}, in the {\tt main}
function we only instrumented the call to \texttt{do\_sort} with
randomly generated data because it's the only one we care. To increase
the granularity, and to understand better what's going on with the
recursive calls we also instrumented {\tt multisort} with appropriate
names ({\tt multisort-i}, {\tt merge-n-i}, {\tt basicsort}). We did a
similar thing for merge.

\vspace{0.5cm}
\begin{figure}[h!]
  \center
  \includegraphics[width=1.0\textwidth,trim={2cm 0 0 0},clip]{figs/multisort_tareador.pdf}
  \caption{Detail of the output of tareador for
    multisort-tareador. The full graphic is in
    {\tt multisort\_tareador.pdf}.}
  \label{fig:multisort-tareador}
\end{figure}
\vspace{0.5cm}

From the figure \ref{fig:multisort-tareador} we can see how the {\tt
  basicsort}s are fully parallelizable. The two merges of size n/4 for
every call to {\tt multisort} are also mutually parallelizable. They
only depend on the sorting of the relevant subarray. The call for size n/2 has to wait
for them though.

\item
  \textbf{Write a table with the execution time and speed-up predicted
    by Dimemas (for 1, 2, 4, 8, 16, 32 and 64 processors) for the task
    decomposition specified with Tareador. Are the results close to the
    ideal case? Reason about your answer.}

\begin{figure}[h!]
  \center
\begin{tabular}{| c || c | c |}
\hline
\textbf{N} & \textbf{Time (s)} & \textbf{Speedup}
\\
\hline
\hline
 1 & 53.843 &     - \\ \hline
 2 & 30.379 & 1.772 \\ \hline
 4 & 18.592 & 2.895 \\ \hline
 8 & 12.701 & 4.239 \\ \hline
16 & 10.091 & 5.335 \\ \hline
32 &  8.895 & 6.053 \\ \hline
64 &  8.326 & 6.467 \\ \hline

\end{tabular}
\caption{Table for times and speedup as a function of the number of threads (N)}
\label{fig:speedup-table}
\end{figure}

In figure \ref{fig:speedup-table} we can see that the scalability
degrades past 8 threads. This result is expected since due to
data dependencies, the program is not fully parallelizable (being the
merging the bottleneck). The critical path for mergesort is {\tt
  basicsort} $\rightarrow$ {\tt merge n/4} $\rightarrow$ {\tt merge n/2}. We can see in the
following graphics how the merging is being the bottleneck as we
increase the number of threads (clearly noticeable with 32 threads) while
the {\tt basicsort}ing (in pink) is clearly fully parallelizable.

Probably it is more scalable than reported by Dimemas, because Tareador is
taking into account the initialization of the array, which is sequential.

\vspace{0.5cm}
\begin{figure}[h!]
  \center
  \includegraphics[width=1.0\textwidth]{figs/paraver_2_cores.png}
  \caption{Dimemas simulation with 2 cores}
\end{figure}

\vspace{0.5cm}
\begin{figure}[h!]
  \center
  \includegraphics[width=1.0\textwidth]{figs/paraver_8_cores.png}
  \caption{Dimemas simulation with 8 cores}
\end{figure}

\vspace{0.5cm}
\begin{figure}[h!]
  \center
  \includegraphics[width=1.0\textwidth]{figs/paraver_32_cores.png}
  \caption{Dimemas simulation with 32 cores. The red \emph{circle} shows how the
    merging doesn't scale well (although basicsorting does)}
\end{figure}

\end{enumerate}

\newpage
\section{Parallelization with OpenMP}
\begin{enumerate}
  \setcounter{enumi}{2}
  \item
    \textbf{Briefly describe the two versions (Leaf and Tree)
      implemented, making references to the source code included in
      the compressed tar file.}

    The idea in both versions is to create tasks and add them to a pool,
    so the threads execute them when they can.

\subsection{Leaf}
The leaf version creates a task for every base case. We need to
take care of the syncronization.

\subsubsection{merge}
We create a task for every {\tt basicmerge}. There's no need of
synchronization at this level, both merges are totally parallelizable.

\begin{minted}{c}
void merge(long n, T left[n], T right[n], T result[n*2],
           long start, long length) {
    if (length < MIN_MERGE_SIZE*2L) {
       // Base case
       #pragma omp task
       basicmerge(n, left, right, result, start, length);
    } else {
       // Recursive decomposition
       merge(n, left, right, result, start, length/2);
       merge(n, left, right, result, start + length/2, length/2);
    }
}
\end{minted}

\subsubsection{multisort}
We create a task for every {\tt basicsort}. Here some synchronization is needed.
First of all, we cannot start merging until we have the four subparts sorted.
But every multisort is completely independent from another. Also, as we saw on figure
\ref{fig:multisort-tareador} the merge of size n/2 depends of the previous merges, thus
we need to wait after them.

\begin{minted}{c}
void multisort(long n, T data[n], T tmp[n]) {
    if (n >= MIN_SORT_SIZE*4L) {
        // Recursive decomposition
        multisort(n/4L, &data[0], &tmp[0]);
        multisort(n/4L, &data[n/4L], &tmp[n/4L]);
        multisort(n/4L, &data[n/2L], &tmp[n/2L]);
        multisort(n/4L, &data[3L*n/4L], &tmp[3L*n/4L]);
        #pragma omp taskwait

        merge(n/4L, &data[0], &data[n/4L], &tmp[0], 0, n/2L);
        merge(n/4L, &data[n/2L], &data[3L*n/4L], &tmp[n/2L], 0, n/2L);
	#pragma omp taskwait

        merge(n/2L, &tmp[0], &tmp[n/2L], &data[0], 0, n);
    } else {
        #pragma omp task
        basicsort(n, data);
    }
}
\end{minted}

\subsection{Tree}

In the tree version, we create a task for every inner node. In this version
two more {\tt taskwaits} are needed.

\subsubsection{merge}
We create a task for every {\tt merge} call. Notice how the {\tt
  taskwait} here is needed, otherwise, two new {\tt merge}s on the
same subpart of the vector could be started.

\begin{minted}{c}
void merge(long n, T left[n], T right[n], T result[n*2],
           long start, long length) {
    if (length < MIN_MERGE_SIZE*2L) {
        // Base case
        basicmerge(n, left, right, result, start, length);
    } else {
        // Recursive decomposition
        #pragma omp task
        merge(n, left, right, result, start, length/2);
	#pragma omp task
        merge(n, left, right, result, start + length/2, length/2);
	#pragma omp taskwait
    }
}
\end{minted}

\subsubsection{multisort}
We create a task for every {\tt multisort} call. When we are finished
sorting, (hence the {\tt taskwait} we can start the merging. Now, we
create a task for every call to {\tt merge} with size n/4. As in the
leaf case, we need to wait both are merged to start the merging with
size n/2. Notice how the taskwait is needed after the last merge. To
understand this, assume 4 divisions (1 multisort call). If 1 is done
and 0 is still going, a merge of 2 could be started (on the
\emph{upper} level), but it could produce an incorrect result.

\begin{minted}{c}
void multisort(long n, T data[n], T tmp[n]) {
    if (n >= MIN_SORT_SIZE*4L) {
        // Recursive decomposition
	#pragma omp task
        multisort(n/4L, &data[0], &tmp[0]);
	#pragma omp task
        multisort(n/4L, &data[n/4L], &tmp[n/4L]);
	#pragma omp task
        multisort(n/4L, &data[n/2L], &tmp[n/2L]);
	#pragma omp task
        multisort(n/4L, &data[3L*n/4L], &tmp[3L*n/4L]);
	#pragma omp taskwait

	#pragma omp task
        merge(n/4L, &data[0], &data[n/4L], &tmp[0], 0, n/2L);
	#pragma omp task
        merge(n/4L, &data[n/2L], &data[3L*n/4L], &tmp[n/2L], 0, n/2L);
	#pragma omp taskwait

        merge(n/2L, &tmp[0], &tmp[n/2L], &data[0], 0, n);
	#pragma omp taskwait
    } else {
        // Base case
	basicsort(n, data);
    }
}
\end{minted}

It's important to note that both versions are probably not technically
optimal.  When we are done with the two first multisorts, we could
start the first merge of size n/4. We avoided this, to benefit from
the simplicity of this code.
\end{enumerate}

\section{Performance analysis}
\textbf{Write a text, inspired in the one provided below, summarizing
  the performance results for the two versions (Leaf and Tree) of
  multisort.}

The performance of the two parallelization strategies for multisort
has been analyzed on a multiprocessor architecture with 12 Intel
cores, distributed in 2 sockets (6 cores each). Using hyperthreading,
it has 2 threads per core. It has 3 cache levels: 32 KB L1, 256 KB L2
(both private per core) and a 12 MB L3 shared between cores of the
same socket. It has a main memory totalling 23.49 GB.

For all the performance results we have used an input vector of size
8192 Kelements randomly generated by the program itself.  The program
performs several sort steps in order to verify the influence of the
randomness of the data in the input vector.

The recursive depth of the multisort algorithm is controlled with the
third and fourht parameters passed to the executable ({\tt argv[2]}
and {\tt argv[3]}), that modifies the variables {\tt MIN\_SORT\_SIZE}
and {\tt MIN\_MERGE\_SIZE}. Using diferent limit sizes in the tree
version, we obtain diferent times as shown by figure
\ref{fig:min-sort}.

\vspace{0.5cm}
\begin{figure}[h!]
  \center
  \includegraphics[width=1.0\textwidth]{figs/min_sort_size.pdf}
  \caption{As the size limit grows it takes more time (due to the
    overhead of the task creation).}
  \label{fig:min-sort}
\end{figure}
\vspace{0.5cm}

 The first conclusion of the analysis is that the tree version has
 better scalability than the leaf version. This is due to the fact
 that the parallelism exploited in the leaf version is limited by the
 waiting of the merges. The basicsorting tasks will not be really
 created until the current subtree has been sorted. This critical path
 is reached much earlier than in the tree version, were there are
 always tasks to be done.

\vspace{0.5cm}
\begin{figure}[h!]
  \center
  \includegraphics[width=1.0\textwidth]{figs/paraver-leaf.png}
  \includegraphics[width=1.0\textwidth]{figs/paraver-tree.png}
  \caption{Tree version is faster than leaf version. In white, the
    time spent on merging.}
  \label{fig:paraver-double}
\end{figure}
\vspace{0.5cm}


\begin{figure}[h!]

  \center
  \begin{tabular}{| c || c | c | c |}
    \hline
    \multicolumn{4}{| c |}{\bf Leaf}\\
    \hline
     {\bf N} & {\bf 8192 Ke} & {\bf 16384 Ke} & {\bf 32768 Ke}\\
    \hline
     1 &        1 &        1 &        1 \\ \hline
     2 & 1.874 & 1.897 & 1.826 \\ \hline
     4 & 3.741 & 3.786 & 3.763 \\ \hline
     6 & 3.830 & 3.798 & 3.910 \\ \hline
     8 & 3.879 & 3.888 & 3.990 \\ \hline
    10 & 3.771 & 3.971 & 4.025 \\ \hline
    12 & 3.688 & 3.937 & 3.918 \\ \hline
    \multicolumn{4}{| c |}{\bf Tree}\\
    \hline
     {\bf N} & {\bf 8192 Ke} & {\bf 16384 Ke} & {\bf 32768 Ke}\\
    \hline
     1 &        1 &        1 &        1 \\ \hline
     2 & 1.935 & 1.938 & 1.925 \\ \hline
     4 & 3.768 & 3.757 & 3.800 \\ \hline
     6 & 3.517 & 4.061 & 4.414 \\ \hline
     8 & 5.081 & 5.174 & 6.031 \\ \hline
    10 & 6.150 & 5.224 & 7.145 \\ \hline
    12 & 6.577 & 7.088 & 8.052 \\ \hline
  \end{tabular}
  \label{fig:speedups}
  \caption{We can see that tree scales better than leaf. Even better
    with bigger sizes.}
\end{figure}


Figure \ref{fig:paraver-double} shows the execution timelines
visualized with Paraver, supporting the previous argument.  Figure 8
shows the speedup, with respect to the sequential, for different
vector sizes (8, 16 and 32 Megaelements), for the two OpenMP versions
and for the different invocations of multisort in the main program.
\end{document}
