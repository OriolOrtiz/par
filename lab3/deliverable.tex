\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[usenames,dvipsnames]{color}
\usepackage{graphicx}
\usepackage[justification=centering,labelfont=bf]{caption}
\usepackage{listings}
\usepackage{minted}
\usepackage[hidelinks]{hyperref}
\begin{document}
\begin{titlepage}
\begin{center}
\textsc{\Large Parallelism}
\\
\texttt{1202}
\\[1.5cm]
\rule{\linewidth}{0.5mm}
\\[0.4cm]
{\huge
\bfseries
Lab 3: Divide and conquer parallelism with OpenMP - Sorting
\\[0.4cm]
}
\rule{\linewidth}{0.5mm}
\\[2.5cm]
\begin{minipage}{0.4\textwidth}
\begin{flushleft}
\large
Héctor Ramón Jiménez
\end{flushleft}
\end{minipage}
\begin{minipage}{0.4\textwidth}
\begin{flushright}
\large
Alvaro Espuña Buxó
\end{flushright}
\end{minipage}
\vfill
{\large
\today
}
\\
{\large
\texttt{Facultat d'Informàtica de Barcelona}
}
\end{center}
\end{titlepage}
\section{Analysis with Tareador}
\begin{enumerate}
\setcounter{enumi}{0}
\item
\textbf{Include the source of the sequential multisort.c code modified
  with the calls to the Tareador API and the task graph generated.}

As we can see in {\tt multisort-tareador.c}, in the {\tt main}
function we only instrumented the call to \texttt{do\_sort} with
randomly generated data because it's the only one we care. To increase
the granularity, and to understand better what's going on with the
recursive calls we also instrumented {\tt multisort} with appropriate
names ({\tt multisort-i}, {\tt merge-n-i}, {\tt basicsort}). We did a
similar thing for merge.

\vspace{0.5cm}
\begin{figure}[h!]
  \center
  \includegraphics[width=1.0\textwidth,trim={2cm 0 0 0},clip]{figs/multisort_tareador.pdf}
  \caption{Detail of the output of tareador for
    multisort-tareador. The full graphic is in
    {\tt multisort\_tareador.pdf}.}
  \label{fig:multisort-tareador}
\end{figure}
\vspace{0.5cm}

From the figure \ref{fig:multisort-tareador} we can see how the {\tt
  basicsort}s are fully parallelizable. The two merges of size n/4 for
every call to {\tt multisort} are also mutually parallelizable. They
only depend on the sorting of the relevant subarray. The call for size n/2 has to wait
for them though.

\item
  \textbf{Write a table with the execution time and speed-up predicted
    by Dimemas (for 1, 2, 4, 8, 16, 32 and 64 processors) for the task
    decomposition specified with Tareador. Are the results close to the
    ideal case? Reason about your answer.}

\begin{figure}[h!]
  \center
\begin{tabular}{| c || c | c |}
\hline
\textbf{N} & \textbf{Time (s)} & \textbf{Speedup}
\\
\hline
\hline
 1 & 53.843 &     - \\ \hline
 2 & 30.379 & 1.772 \\ \hline
 4 & 18.592 & 2.895 \\ \hline
 8 & 12.701 & 4.239 \\ \hline
16 & 10.091 & 5.335 \\ \hline
32 &  8.895 & 6.053 \\ \hline
64 &  8.326 & 6.467 \\ \hline

\end{tabular}
\caption{Table for times and speedup as a function of the number of threads (N)}
\label{fig:speedup-table}
\end{figure}

In figure \ref{fig:speedup-table} we can see that the scalability
degrades past 8 threads. This result is expected since due to
data dependencies, the program is not fully parallelizable (being the
merging the bottleneck). The critical path for mergesort is {\tt
  basicsort} $\rightarrow$ {\tt merge n/4} $\rightarrow$ {\tt merge n/2}. We can see in the
following graphics how the merging is being the bottleneck as we
increase the number of threads (clearly noticeable with 32 threads) while
the {\tt basicsort}ing (in pink) is clearly fully parallelizable.

Probably it is more scalable than reported by Dimemas, because Tareador is
taking into account the initialization of the array, which is sequential.

\vspace{0.5cm}
\begin{figure}[h!]
  \center
  \includegraphics[width=1.0\textwidth]{figs/paraver_2_cores.png}
  \caption{Dimemas simulation with 2 cores}
\end{figure}

\vspace{0.5cm}
\begin{figure}[h!]
  \center
  \includegraphics[width=1.0\textwidth]{figs/paraver_8_cores.png}
  \caption{Dimemas simulation with 8 cores}
\end{figure}

\vspace{0.5cm}
\begin{figure}[h!]
  \center
  \includegraphics[width=1.0\textwidth]{figs/paraver_32_cores.png}
  \caption{Dimemas simulation with 32 cores. The red \emph{circle} shows how the
    merging doesn't scale well (although basicsorting does)}
\end{figure}

\end{enumerate}

\newpage
\section{Parallelization with OpenMP}
\begin{enumerate}
  \setcounter{enumi}{2}
  \item
    \textbf{Briefly describe the two versions (Leaf and Tree)
      implemented, making references to the source code included in
      the compressed tar file.}

\subsection*{Leaf}

\subsection*{Tree}

\end{enumerate}

\section{Performance analysis}
\textbf{Write a text, inspired in the one provided below, summarizing
  the performance results for the two versions (Leaf and Tree) of
  multisort.}

The performance of the two parallelization strategies for multisort
has been analyzed on a multiprocessor architecture with 12 Intel
cores ... /* complete this paragraph with the information you got in
the first laboratory assignment */.  For all the performance results we
have used an input vector of size XXX randomly generated by the
program itself. The program performs several sort steps in order to
verify the influence of the randomness of the data in the input
vector. The recursive depth of the multisort algorithm is controlled
with /* complete this paragraph with the appropriate information */
The first conclusion of the analysis is that version XXX has better
scalability than version YYY. This is due to the fact that the
parallelism exploited in version YYY is limited by /* complete the
paragraph with an appropriate reasoning */. Figure 1 shows the
execution timelines visualized with Paraver, supporting the previous
argument.  Figure 2 plots the speedup, with respect to the sequential,
for different vector sizes (8, 16 and 32 Megaelements), for the two
OpenMP versions and for the different invocations of multisort in the
main program.

\end{document}
